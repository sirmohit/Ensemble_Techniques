{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb40c37f",
   "metadata": {},
   "source": [
    "Q1. You are working on a machine learning project where you have a dataset containing numerical and categorical features. You have identified that some of the features are highly correlated and there are missing values in some of the columns. You want to build a pipeline that automates the feature engineering process and handles the missing values\n",
    "\n",
    "Design a pipeline that includes the following steps\"\n",
    "Use an automated feature selection method to identify the important features in the datasets\n",
    "Create a numerical pipeline that includes the following steps\"\n",
    "Impute the missing values in the numerical columns using the mean of the column values\n",
    "Scale the numerical columns using standardization\n",
    "Create a categorical pipeline that includes the following steps\"\n",
    "Impute the missing values in the categorical columns using the most frequent value of the columns\n",
    "One-hot encodes the categorical columns\n",
    "Combine the numerical and categorical pipelines using a ColumnTransformers\n",
    "Use a Random Forest Classifier to build the final models\n",
    "Evaluate the accuracy of the model on the test dataset\n",
    "Note! Your solution should include code snippets for each step of the pipeline and a brief explanation of each step. You should also provide an interpretation of the results and suggest possible improvements for the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa750f6c",
   "metadata": {},
   "source": [
    "Necessary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afdd7dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score,recall_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8a53d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "X, y = data.data, data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1d64e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a67a61",
   "metadata": {},
   "source": [
    "1. Use an automated feature selection method to identify the important features in the dataset.\n",
    "\n",
    "\n",
    "Feature selection helps identify the most relevant features for your model, improving performance and reducing overfitting. You can use methods like correlation analysis, recursive feature elimination, or feature importance from tree-based models. Here's an example using SelectKBest with f_classif scoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39674f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Use an automated feature selection method\n",
    "selector = SelectKBest(score_func=f_classif, k=3)\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9cdb33d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.6, 1. , 0.2],\n",
       "       [5.7, 1.5, 0.4],\n",
       "       [6.7, 4.4, 1.4],\n",
       "       [4.8, 1.6, 0.2],\n",
       "       [4.4, 1.3, 0.2],\n",
       "       [6.3, 5. , 1.9],\n",
       "       [6.4, 4.5, 1.5],\n",
       "       [5.2, 1.5, 0.2],\n",
       "       [5. , 1.4, 0.2],\n",
       "       [5.2, 1.5, 0.1],\n",
       "       [5.8, 5.1, 1.9],\n",
       "       [6. , 4.5, 1.6],\n",
       "       [6.7, 4.7, 1.5],\n",
       "       [5.4, 1.3, 0.4],\n",
       "       [5.4, 1.5, 0.2],\n",
       "       [5.5, 3.7, 1. ],\n",
       "       [6.3, 5.1, 1.5],\n",
       "       [6.4, 5.5, 1.8],\n",
       "       [6.6, 4.4, 1.4],\n",
       "       [7.2, 6.1, 2.5],\n",
       "       [5.7, 4.2, 1.3],\n",
       "       [7.6, 6.6, 2.1],\n",
       "       [5.6, 4.5, 1.5],\n",
       "       [5.1, 1.4, 0.2],\n",
       "       [7.7, 6.7, 2. ],\n",
       "       [5.8, 4.1, 1. ],\n",
       "       [5.2, 1.4, 0.2],\n",
       "       [5. , 1.3, 0.3],\n",
       "       [5.1, 1.9, 0.4],\n",
       "       [5. , 3.5, 1. ],\n",
       "       [6.3, 4.9, 1.8],\n",
       "       [4.8, 1.9, 0.2],\n",
       "       [5. , 1.6, 0.2],\n",
       "       [5.1, 1.7, 0.5],\n",
       "       [5.6, 4.2, 1.3],\n",
       "       [5.1, 1.5, 0.2],\n",
       "       [5.7, 4.2, 1.2],\n",
       "       [7.7, 6.7, 2.2],\n",
       "       [4.6, 1.4, 0.2],\n",
       "       [6.2, 4.3, 1.3],\n",
       "       [5.7, 5. , 2. ],\n",
       "       [5.5, 1.4, 0.2],\n",
       "       [6. , 4.8, 1.8],\n",
       "       [5.8, 5.1, 1.9],\n",
       "       [6. , 4. , 1. ],\n",
       "       [5.4, 4.5, 1.5],\n",
       "       [6.2, 5.4, 2.3],\n",
       "       [5.5, 4. , 1.3],\n",
       "       [5.4, 1.7, 0.4],\n",
       "       [5. , 3.3, 1. ],\n",
       "       [6.4, 5.3, 1.9],\n",
       "       [5. , 1.4, 0.2],\n",
       "       [5. , 1.2, 0.2],\n",
       "       [5.5, 3.8, 1.1],\n",
       "       [6.7, 5. , 1.7],\n",
       "       [4.9, 1.5, 0.2],\n",
       "       [5.8, 5.1, 2.4],\n",
       "       [5. , 1.5, 0.2],\n",
       "       [5. , 1.6, 0.6],\n",
       "       [5.9, 4.8, 1.8],\n",
       "       [5.1, 3. , 1.1],\n",
       "       [6.9, 5.7, 2.3],\n",
       "       [6. , 5.1, 1.6],\n",
       "       [6.1, 5.6, 1.4],\n",
       "       [7.7, 6.1, 2.3],\n",
       "       [5.5, 4. , 1.3],\n",
       "       [4.4, 1.4, 0.2],\n",
       "       [4.3, 1.1, 0.1],\n",
       "       [6. , 5. , 1.5],\n",
       "       [7.2, 6. , 1.8],\n",
       "       [4.6, 1.5, 0.2],\n",
       "       [5.1, 1.4, 0.3],\n",
       "       [4.4, 1.3, 0.2],\n",
       "       [6.3, 4.9, 1.5],\n",
       "       [6.3, 5.6, 2.4],\n",
       "       [4.6, 1.4, 0.3],\n",
       "       [6.8, 5.5, 2.1],\n",
       "       [6.3, 6. , 2.5],\n",
       "       [4.7, 1.3, 0.2],\n",
       "       [6.1, 4.7, 1.4],\n",
       "       [6.5, 4.6, 1.5],\n",
       "       [6.2, 4.8, 1.8],\n",
       "       [7. , 4.7, 1.4],\n",
       "       [6.4, 5.3, 2.3],\n",
       "       [5.1, 1.6, 0.2],\n",
       "       [6.9, 5.4, 2.1],\n",
       "       [5.9, 4.2, 1.5],\n",
       "       [6.5, 5.2, 2. ],\n",
       "       [5.7, 3.5, 1. ],\n",
       "       [5.2, 3.9, 1.4],\n",
       "       [6.1, 4.6, 1.4],\n",
       "       [4.5, 1.3, 0.3],\n",
       "       [6.6, 4.6, 1.3],\n",
       "       [5.5, 4.4, 1.2],\n",
       "       [5.3, 1.5, 0.2],\n",
       "       [5.6, 4.1, 1.3],\n",
       "       [7.3, 6.3, 1.8],\n",
       "       [6.7, 5.7, 2.1],\n",
       "       [5.1, 1.5, 0.4],\n",
       "       [4.9, 3.3, 1. ],\n",
       "       [6.7, 5.7, 2.5],\n",
       "       [7.2, 5.8, 1.6],\n",
       "       [4.9, 1.4, 0.1],\n",
       "       [6.7, 5.6, 2.4],\n",
       "       [4.9, 1.4, 0.2],\n",
       "       [6.9, 4.9, 1.5],\n",
       "       [7.4, 6.1, 1.9],\n",
       "       [6.3, 5.6, 1.8],\n",
       "       [5.7, 4.1, 1.3],\n",
       "       [6.5, 5.5, 1.8],\n",
       "       [6.3, 4.4, 1.3],\n",
       "       [6.4, 4.3, 1.3],\n",
       "       [5.6, 4.9, 2. ],\n",
       "       [5.9, 5.1, 1.8],\n",
       "       [5.4, 1.7, 0.2],\n",
       "       [6.1, 4. , 1.3],\n",
       "       [4.9, 4.5, 1.7],\n",
       "       [5.8, 1.2, 0.2],\n",
       "       [5.8, 4. , 1.2],\n",
       "       [7.1, 5.9, 2.1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1249346a",
   "metadata": {},
   "source": [
    "2. Create a numerical pipeline that includes the following steps:\n",
    "a. Impute the missing values in the numerical columns using the mean of the column values.\n",
    "b. Scale the numerical columns using standardization. Imputing missing values fills gaps in the dataset, and scaling numerical features standardizes them for better model performance. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bb09cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create a numerical pipeline\n",
    "num_pipeline=Pipeline([\n",
    "    ('imputer',SimpleImputer(strategy='mean')),\n",
    "    ('scaler',StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f4fa32",
   "metadata": {},
   "source": [
    "3. Create a categorical pipeline that includes the following steps:\n",
    "a. Impute the missing values in the categorical columns using the most frequent value of the column.\n",
    "b. One-hot encode the categorical columns. Imputing missing values replaces them with the most frequent value, and one-hot encoding converts categorical variables into binary vectors. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1851bcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create a categorical pipeline\n",
    "cat_pipeline=Pipeline([\n",
    "    ('imputer',SimpleImputer(strategy='most_frequent')),\n",
    "    ('encode',OneHotEncoder())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3b872a",
   "metadata": {},
   "source": [
    "4. Combine the numerical and categorical pipelines using a ColumnTransformer.\n",
    "\n",
    "\n",
    "The ColumnTransformer allows different transformations on different columns, combining them into a single output. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc64332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Combine numerical and categorical pipelines\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, [0, 1, 2, 3]),  # Assuming all columns are numerical\n",
    "    ('cat', cat_pipeline, [])  # Assuming no categorical columns in this dataset\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5de8a95",
   "metadata": {},
   "source": [
    "5. Use a Random Forest Classifier to build the final model.\n",
    "\n",
    "\n",
    "Random Forest is an ensemble learning method that combines multiple decision trees for predictions. It handles both numerical and categorical features and is suitable for classification tasks. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1b0cd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Use a Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c20fb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Create the final pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', rf_classifier)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cab11826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  ('scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  [0, 1, 2, 3]),\n",
       "                                                 ('cat',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(strategy='most_frequent')),\n",
       "                                                                  ('encode',\n",
       "                                                                   OneHotEncoder())]),\n",
       "                                                  [])])),\n",
       "                ('classifier', RandomForestClassifier())])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 7: Train the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c61fe77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the accuracy on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5c314f",
   "metadata": {},
   "source": [
    "Interpretation of Results and Possible Improvements:\n",
    "\n",
    "The pipeline automates feature engineering and missing value handling for numerical and categorical features, leading to a Random Forest model.\n",
    "Evaluate the model's accuracy on the test dataset to assess its performance.\n",
    "If the accuracy is unsatisfactory, consider the following:\n",
    "Hyperparameter Tuning: Experiment with different hyperparameter values for the Random Forest Classifier. Adjusting parameters such as the number of estimators, maximum depth, and minimum samples per leaf can significantly impact the model's performance.\n",
    "\n",
    "Feature Engineering: Explore different feature engineering techniques to enhance the model's performance. For example, you can try creating new features based on domain knowledge, applying different transformations to the existing features, or incorporating interaction terms.\n",
    "\n",
    "Alternative Models: Consider trying alternative models or ensemble methods besides Random Forest. Different algorithms might have different strengths and weaknesses, so exploring models like Gradient Boosting, Support Vector Machines, or Neural Networks could potentially yield better results.\n",
    "\n",
    "Data Augmentation: If the dataset is small, you can explore techniques like data augmentation to generate additional synthetic samples. This can help improve the model's generalization and performance.\n",
    "\n",
    "Cross-Validation: Evaluate the model's performance using cross-validation techniques to obtain more reliable estimates of its accuracy. This can help assess whether the initial accuracy score is consistent across different folds of the data.\n",
    "\n",
    "Handling Class Imbalance: If the dataset has class imbalance, consider addressing it by using techniques such as oversampling, undersampling, or using class weights during model training. This can prevent the model from being biased towards the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1668eb2f",
   "metadata": {},
   "source": [
    "Q2. Build a pipeline that includes a random forest classifier and a logistic regression classifier, and then use a voting classifier to combine their predictions. Train the pipeline on the Iris dataset and evaluate its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f066db76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create individual classifiers\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "lr_classifier = LogisticRegression(random_state=42)\n",
    "\n",
    "# Create a voting classifier\n",
    "voting_classifier = VotingClassifier(\n",
    "    estimators=[('rf', rf_classifier), ('lr', lr_classifier)],\n",
    "    voting='hard'  # Use majority voting\n",
    ")\n",
    "\n",
    "# Create the final pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', voting_classifier)\n",
    "])\n",
    "\n",
    "# Train the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the accuracy on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42748245",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
